Setting up the environment...
Job started on gl1524.arc-ts.umich.edu at Wed Dec 24 23:43:38 EST 2025
Python version: Python 3.9.7
PyTorch version: 2.8.0+cu128
CUDA available: True
Current GPU: NVIDIA A40
NVIDIA A40
----------------------------------------------------------------
EXPERIMENT NAME: supcon_geodesic_dist
W1224 23:43:48.952636 3459522 torch/distributed/run.py:774] 
W1224 23:43:48.952636 3459522 torch/distributed/run.py:774] *****************************************
W1224 23:43:48.952636 3459522 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1224 23:43:48.952636 3459522 torch/distributed/run.py:774] *****************************************
[W1224 23:43:48.811468394 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [localhost]:44009 (errno: 97 - Address family not supported by protocol).
[W1224 23:43:48.812547886 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [localhost]:44009 (errno: 97 - Address family not supported by protocol).
[W1224 23:43:48.813610391 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [localhost]:44009 (errno: 97 - Address family not supported by protocol).
[W1224 23:43:49.093279223 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [gl1524.arc-ts.umich.edu]:40141 (errno: 97 - Address family not supported by protocol).
[W1224 23:43:59.654863501 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [gl1524.arc-ts.umich.edu]:40141 (errno: 97 - Address family not supported by protocol).
[W1224 23:43:59.655029007 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [gl1524.arc-ts.umich.edu]:40141 (errno: 97 - Address family not supported by protocol).
=== CONFIG ===
MODEL_NAME=facebook/wav2vec2-xls-r-300m
SAVE_DIR=/home/jsudan/wav2vec_contr_loss/checkpoints_stage1/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m
TRAIN_ROOT=/nfs/turbo/umd-hafiz/issf_server_data/AsvSpoofData_2019/train/LA/ASVspoof2019_LA_train/flac
TRAIN_PROTOCOL=/nfs/turbo/umd-hafiz/issf_server_data/AsvSpoofData_2019/train/LA/ASVspoof2019_train_protocol_with_speaker.txt
DEV_ROOT=/nfs/turbo/umd-hafiz/issf_server_data/AsvSpoofData_2019/train/LA/ASVspoof2019_LA_dev/flac
DEV_PROTOCOL=/nfs/turbo/umd-hafiz/issf_server_data/AsvSpoofData_2019/train/LA/ASVspoof2019_dev_protocol_with_speaker.txt
TARGET_SAMPLE_RATE=16000
MAX_DURATION_SECONDS=5
INPUT_DIM=1024
HIDDEN_DIM=256
DROPOUT=0.1
EPOCHS=100
BATCH_SIZE=32
NUM_SAMPLES=None
HEAD_LR=0.005
ENC_LR=1e-05
WEIGHT_DECAY=0.003
TEMPERATURE=0.2
NUM_WORKERS=4
SEED=1337
UNIFORMITY_WEIGHT=0.0
UNIFORMITY_T=2.0
SUPCON_SIMILARITY=geodesic
TOPK_NEG=15
WARMUP_EPOCHS=100
ALPHA_END=1.0
ALPHA_RAMP_EPOCHS=80
USE_RAWBOOST=True
RAWBOOST_PROB=0.7
FINETUNE_ENCODER=True
DISTRIBUTED=True | WORLD_SIZE=2 | RANK=0
=============
Using device: cuda:0 | RawBoost=True (p=0.7)
CUDA device count: 2
[rank1]:[W1224 23:44:12.981273473 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W1224 23:44:12.981466655 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[E1224 23:57:45.023213685 ProcessGroupNCCL.cpp:685] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3452, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.
[rank1]:[E1224 23:57:45.029713146 ProcessGroupNCCL.cpp:2252] [PG ID 0 PG GUID 0(default_pg) Rank 1]  failure detected by watchdog at work sequence id: 3452 PG status: last enqueued work: 3452, last completed work: 3451
[rank1]:[E1224 23:57:45.029730532 ProcessGroupNCCL.cpp:732] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1224 23:57:45.029777268 ProcessGroupNCCL.cpp:2584] [PG ID 0 PG GUID 0(default_pg) Rank 1] First PG on this rank to signal dumping.
[rank1]:[E1224 23:57:45.079639945 ProcessGroupNCCL.cpp:1870] [PG ID 0 PG GUID 0(default_pg) Rank 1] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 3452, last completed NCCL work: 3451.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E1224 23:57:45.079825017 ProcessGroupNCCL.cpp:1806] [PG ID 0 PG GUID 0(default_pg) Rank 0] Observed flight recorder dump signal from another rank via TCPStore.
[rank0]:[E1224 23:57:45.079947599 ProcessGroupNCCL.cpp:1870] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from  rank 1 and we will try our best to dump the debug info. Last enqueued NCCL work: 3490, last completed NCCL work: 3450.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank1]:[E1224 23:57:45.080159258 ProcessGroupNCCL.cpp:1589] [PG ID 0 PG GUID 0(default_pg) Rank 1] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E1224 23:57:45.080161932 ProcessGroupNCCL.cpp:1589] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E1224 23:57:46.554261856 ProcessGroupNCCL.cpp:685] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3451, OpType=ALLREDUCE, NumelIn=256, NumelOut=256, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
[rank0]:[E1224 23:57:46.554394154 ProcessGroupNCCL.cpp:2252] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 3451 PG status: last enqueued work: 3490, last completed work: 3450
[rank0]:[E1224 23:57:46.554400983 ProcessGroupNCCL.cpp:732] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank1]:[E1224 23:58:45.534334694 ProcessGroupNCCL.cpp:746] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E1224 23:58:45.534365444 ProcessGroupNCCL.cpp:760] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E1224 23:58:45.536725137 ProcessGroupNCCL.cpp:2068] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3452, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:688 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x14a256f81eb0 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x14a257ee8147 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1591 (0x14a257eebb61 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xd2 (0x14a257eecec2 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc9039 (0x14a23b7b8039 in /sw/pkgs/arc/python3.9-anaconda/2021.11/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x14a2b569a1ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x14a2b52f58d3 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3452, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600084 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:688 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x14a256f81eb0 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x14a257ee8147 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1591 (0x14a257eebb61 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xd2 (0x14a257eecec2 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc9039 (0x14a23b7b8039 in /sw/pkgs/arc/python3.9-anaconda/2021.11/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x14a2b569a1ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x14a2b52f58d3 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2074 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x14a256f81eb0 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe1c1a1 (0x14a257ec41a1 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9468e6 (0x14a2579ee8e6 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xc9039 (0x14a23b7b8039 in /sw/pkgs/arc/python3.9-anaconda/2021.11/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0x81ca (0x14a2b569a1ca in /lib64/libpthread.so.0)
frame #5: clone + 0x43 (0x14a2b52f58d3 in /lib64/libc.so.6)

[rank0]:[E1224 23:58:47.238444463 ProcessGroupNCCL.cpp:746] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E1224 23:58:47.238470533 ProcessGroupNCCL.cpp:760] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E1224 23:58:47.241895913 ProcessGroupNCCL.cpp:2068] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3451, OpType=ALLREDUCE, NumelIn=256, NumelOut=256, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:688 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x14859b103eb0 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x14859c06a147 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1591 (0x14859c06db61 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xd2 (0x14859c06eec2 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc9039 (0x14857f93a039 in /sw/pkgs/arc/python3.9-anaconda/2021.11/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x1485f981c1ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x1485f94778d3 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3451, OpType=ALLREDUCE, NumelIn=256, NumelOut=256, Timeout(ms)=600000) ran for 600030 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:688 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x14859b103eb0 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x14859c06a147 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1591 (0x14859c06db61 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xd2 (0x14859c06eec2 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xc9039 (0x14857f93a039 in /sw/pkgs/arc/python3.9-anaconda/2021.11/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x1485f981c1ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x1485f94778d3 in /lib64/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2074 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x14859b103eb0 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe1c1a1 (0x14859c0461a1 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9468e6 (0x14859bb708e6 in /home/jsudan/myenv/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xc9039 (0x14857f93a039 in /sw/pkgs/arc/python3.9-anaconda/2021.11/bin/../lib/libstdc++.so.6)
frame #4: <unknown function> + 0x81ca (0x1485f981c1ca in /lib64/libpthread.so.0)
frame #5: clone + 0x43 (0x1485f94778d3 in /lib64/libc.so.6)

W1224 23:58:54.610507 3459522 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 3459591 closing signal SIGTERM
E1224 23:58:55.075897 3459522 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: -6) local_rank: 1 (pid: 3459592) of binary: /home/jsudan/myenv/bin/python
Traceback (most recent call last):
  File "/home/jsudan/myenv/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/jsudan/myenv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/home/jsudan/myenv/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/jsudan/myenv/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/jsudan/myenv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/jsudan/myenv/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_stage1.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-24_23:58:54
  host      : gl1524.arc-ts.umich.edu
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 3459592)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 3459592
========================================================
Using device: cuda
Model: facebook/wav2vec2-xls-r-300m
Checkpoint: /home/jsudan/wav2vec_contr_loss/checkpoints_stage1/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m/facebook__wav2vec2-xls-r-300m_stage1_head_best.pt
Saving to: /home/jsudan/wav2vec_contr_loss/plots/dep_embeddings/ASV/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m
Collecting embeddings on eval set...
/home/jsudan/myenv/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/jsudan/myenv/lib/python3.9/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12040. The TBB threading layer is disabled.
  warnings.warn(problem)
  Processed 5120 samples...
  Processed 10240 samples...
  Processed 15360 samples...
  Processed 20480 samples...
  Processed 25600 samples...
  Processed 30720 samples...
  Processed 35840 samples...
  Processed 40960 samples...
  Processed 46080 samples...
  Processed 51200 samples...
  Processed 56320 samples...
  Processed 61440 samples...
  Processed 66560 samples...
Total eval embeddings: 71237 (dim=256)
Running UMAP...
Saving PNG plot...
Saved PNG: /home/jsudan/wav2vec_contr_loss/plots/dep_embeddings/ASV/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m/stage1_umap_eval_by_attack.png
Saving interactive HTML plot...
Saved HTML: /home/jsudan/wav2vec_contr_loss/plots/dep_embeddings/ASV/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m/stage1_umap_eval_by_attack.html
Done.
Using device: cuda
Model: facebook/wav2vec2-xls-r-300m
Checkpoint: /home/jsudan/wav2vec_contr_loss/checkpoints_stage1/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m/facebook__wav2vec2-xls-r-300m_stage1_head_best.pt
Saving to: /home/jsudan/wav2vec_contr_loss/plots/dep_embeddings/ITW/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m
Collecting embeddings on ITW set...
/home/jsudan/myenv/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/jsudan/myenv/lib/python3.9/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12040. The TBB threading layer is disabled.
  warnings.warn(problem)
  Processed 5120 samples...
  Processed 10240 samples...
  Processed 15360 samples...
  Processed 20480 samples...
  Processed 25600 samples...
  Processed 30720 samples...
Total ITW embeddings: 31779 (dim=256)
Running UMAP...
Saving PNG plot...
Saved PNG: /home/jsudan/wav2vec_contr_loss/plots/dep_embeddings/ITW/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m/stage1_umap_itw_real_vs_spoof.png
Saving interactive HTML plot...
Saved HTML: /home/jsudan/wav2vec_contr_loss/plots/dep_embeddings/ITW/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m/stage1_umap_itw_real_vs_spoof.html
Done.
Using device: cuda
Loading Stage-1 checkpoint from: /home/jsudan/wav2vec_contr_loss/checkpoints_stage1/supcon_geodesic_dist/facebook__wav2vec2-xls-r-300m/facebook__wav2vec2-xls-r-300m_stage1_head_best.pt
==> Building ASV dataset for split: train
[ASV] Extracting train:   0%|          | 0/100 [00:00<?, ?it/s][ASV] Extracting train:   1%|          | 1/100 [00:06<10:53,  6.60s/it][ASV] Extracting train:   2%|▏         | 2/100 [00:09<07:24,  4.53s/it][ASV] Extracting train:   3%|▎         | 3/100 [00:12<06:16,  3.88s/it][ASV] Extracting train:   4%|▍         | 4/100 [00:15<05:42,  3.57s/it][ASV] Extracting train:   5%|▌         | 5/100 [00:18<05:22,  3.40s/it][ASV] Extracting train:   6%|▌         | 6/100 [00:22<05:09,  3.30s/it][ASV] Extracting train:   7%|▋         | 7/100 [00:25<05:00,  3.23s/it][ASV] Extracting train:   8%|▊         | 8/100 [00:28<04:53,  3.19s/it][ASV] Extracting train:   9%|▉         | 9/100 [00:31<04:48,  3.17s/it][ASV] Extracting train:  10%|█         | 10/100 [00:34<04:43,  3.15s/it][ASV] Extracting train:  11%|█         | 11/100 [00:37<04:39,  3.14s/it][ASV] Extracting train:  12%|█▏        | 12/100 [00:40<04:35,  3.13s/it][ASV] Extracting train:  13%|█▎        | 13/100 [00:43<04:31,  3.12s/it][ASV] Extracting train:  14%|█▍        | 14/100 [00:46<04:28,  3.12s/it][ASV] Extracting train:  15%|█▌        | 15/100 [00:50<04:24,  3.12s/it][ASV] Extracting train:  16%|█▌        | 16/100 [00:53<04:21,  3.12s/it][ASV] Extracting train:  17%|█▋        | 17/100 [00:56<04:18,  3.12s/it][ASV] Extracting train:  18%|█▊        | 18/100 [00:59<04:15,  3.12s/it][ASV] Extracting train:  19%|█▉        | 19/100 [01:02<04:12,  3.12s/it][ASV] Extracting train:  20%|██        | 20/100 [01:05<04:09,  3.12s/it][ASV] Extracting train:  21%|██        | 21/100 [01:08<04:05,  3.11s/it][ASV] Extracting train:  22%|██▏       | 22/100 [01:11<04:02,  3.11s/it][ASV] Extracting train:  23%|██▎       | 23/100 [01:14<03:59,  3.11s/it][ASV] Extracting train:  24%|██▍       | 24/100 [01:18<03:56,  3.11s/it][ASV] Extracting train:  25%|██▌       | 25/100 [01:21<03:53,  3.12s/it][ASV] Extracting train:  26%|██▌       | 26/100 [01:24<03:50,  3.12s/it][ASV] Extracting train:  27%|██▋       | 27/100 [01:27<03:48,  3.13s/it][ASV] Extracting train:  28%|██▊       | 28/100 [01:30<03:45,  3.13s/it][ASV] Extracting train:  29%|██▉       | 29/100 [01:33<03:42,  3.13s/it][ASV] Extracting train:  30%|███       | 30/100 [01:36<03:39,  3.13s/it][ASV] Extracting train:  31%|███       | 31/100 [01:39<03:36,  3.13s/it][ASV] Extracting train:  32%|███▏      | 32/100 [01:43<03:33,  3.13s/it][ASV] Extracting train:  33%|███▎      | 33/100 [01:46<03:29,  3.13s/it][ASV] Extracting train:  34%|███▍      | 34/100 [01:49<03:26,  3.13s/it][ASV] Extracting train:  35%|███▌      | 35/100 [01:52<03:23,  3.13s/it][ASV] Extracting train:  36%|███▌      | 36/100 [01:55<03:20,  3.13s/it][ASV] Extracting train:  37%|███▋      | 37/100 [01:58<03:17,  3.13s/it][ASV] Extracting train:  38%|███▊      | 38/100 [02:01<03:14,  3.13s/it][ASV] Extracting train:  39%|███▉      | 39/100 [02:05<03:11,  3.13s/it][ASV] Extracting train:  40%|████      | 40/100 [02:08<03:07,  3.13s/it][ASV] Extracting train:  41%|████      | 41/100 [02:11<03:04,  3.13s/it][ASV] Extracting train:  42%|████▏     | 42/100 [02:14<03:01,  3.13s/it][ASV] Extracting train:  43%|████▎     | 43/100 [02:17<02:58,  3.13s/it][ASV] Extracting train:  44%|████▍     | 44/100 [02:20<02:55,  3.13s/it][ASV] Extracting train:  45%|████▌     | 45/100 [02:23<02:51,  3.13s/it]slurmstepd: error: *** JOB 38897131 ON gl1524 CANCELLED AT 2025-12-25T00:25:53 ***
